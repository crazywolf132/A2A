use crate::error::A2AResult;
use crate::types::{
    Artifact, Message, Part, Task, TaskState, TaskStatus, TextPart,
};
use anyhow::Context;
use chrono::Utc;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::env;
use tracing::{debug, error, info};
use uuid::Uuid;

pub mod server;

/// OpenAI API response structure
#[derive(Debug, Serialize, Deserialize)]
struct OpenAIResponse {
    id: String,
    object: String,
    created: u64,
    model: String,
    choices: Vec<OpenAIChoice>,
    usage: OpenAIUsage,
}

#[derive(Debug, Serialize, Deserialize)]
struct OpenAIChoice {
    index: u32,
    message: OpenAIMessage,
    finish_reason: String,
}

#[derive(Debug, Serialize, Deserialize)]
struct OpenAIMessage {
    role: String,
    content: String,
}

#[derive(Debug, Serialize, Deserialize)]
struct OpenAIUsage {
    prompt_tokens: u32,
    completion_tokens: u32,
    total_tokens: u32,
}

/// OpenAI API request structure
#[derive(Debug, Serialize, Deserialize)]
struct OpenAIRequest {
    model: String,
    messages: Vec<OpenAIMessage>,
    temperature: f32,
}

/// OpenAI Agent that processes messages using the OpenAI API
pub struct OpenAIAgent {
    client: Client,
    api_key: String,
    model: String,
}

impl OpenAIAgent {
    /// Create a new OpenAI agent
    pub fn new() -> anyhow::Result<Self> {
        dotenv::dotenv().ok();
        
        let api_key = env::var("OPENAI_API_KEY")
            .context("OPENAI_API_KEY environment variable not set")?;
        
        let model = env::var("OPENAI_MODEL").unwrap_or_else(|_| "gpt-3.5-turbo".to_string());
        
        Ok(Self {
            client: Client::new(),
            api_key,
            model,
        })
    }
    
    /// Process a message using the OpenAI API
    pub async fn process_message(&self, message: &str) -> anyhow::Result<String> {
        let request = OpenAIRequest {
            model: self.model.clone(),
            messages: vec![OpenAIMessage {
                role: "user".to_string(),
                content: message.to_string(),
            }],
            temperature: 0.7,
        };
        
        let response = self.client
            .post("https://api.openai.com/v1/chat/completions")
            .header("Authorization", format!("Bearer {}", self.api_key))
            .header("Content-Type", "application/json")
            .json(&request)
            .send()
            .await
            .context("Failed to send request to OpenAI API")?;
        
        if !response.status().is_success() {
            let error_text = response.text().await?;
            return Err(anyhow::anyhow!("OpenAI API error: {}", error_text));
        }
        
        let openai_response: OpenAIResponse = response.json().await
            .context("Failed to parse OpenAI API response")?;
        
        if openai_response.choices.is_empty() {
            return Err(anyhow::anyhow!("OpenAI API returned no choices"));
        }
        
        Ok(openai_response.choices[0].message.content.clone())
    }
    
    /// Handle an A2A task
    pub async fn handle_task(&self, task: Task) -> A2AResult<Task> {
        let task_id = task.id.clone();
        info!("Processing task {}", task_id);
        
        // Extract the message text
        let message_text = task.status.message.as_ref()
            .and_then(|message| {
                message.parts.iter().find_map(|part| {
                    match part {
                        Part::Text(text_part) => Some(text_part.text.clone()),
                        _ => None,
                    }
                })
            })
            .unwrap_or_else(|| "Hello".to_string());
        
        debug!("Processing message: {}", message_text);
        
        // Process the message with OpenAI
        let response = match self.process_message(&message_text).await {
            Ok(response) => response,
            Err(err) => {
                error!("Error processing message: {}", err);
                format!("I'm sorry, I encountered an error: {}", err)
            }
        };
        
        debug!("Generated response: {}", response);
        
        // Create the response task
        let response_message = Message {
            role: "agent".to_string(),
            parts: vec![Part::Text(TextPart {
                part_type: "text".to_string(),
                text: response,
                metadata: None,
            })],
            metadata: None,
        };
        
        let artifact = Artifact {
            name: Some("response".to_string()),
            description: Some("Response generated by OpenAI".to_string()),
            parts: vec![Part::Text(TextPart {
                part_type: "text".to_string(),
                text: "This response was generated using OpenAI's API.".to_string(),
                metadata: None,
            })],
            index: 0,
            append: None,
            metadata: None,
            last_chunk: Some(true),
        };
        
        let mut updated_task = task.clone();
        updated_task.status = TaskStatus {
            state: TaskState::Completed,
            message: Some(response_message),
            timestamp: Utc::now(),
        };
        
        if updated_task.artifacts.is_none() {
            updated_task.artifacts = Some(vec![]);
        }
        
        if let Some(artifacts) = &mut updated_task.artifacts {
            artifacts.push(artifact);
        }
        
        Ok(updated_task)
    }
}
